{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6d49be5-752f-4da3-acd9-9d5bba3820cd",
   "metadata": {
    "collapsed": false,
    "id": "c6d49be5-752f-4da3-acd9-9d5bba3820cd"
   },
   "outputs": [],
   "source": [
    "TESTING_SUBSET = True  # ***** FOR TESTING ONLY !!!!!!!! ******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sv02pJISHp8W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "sv02pJISHp8W",
    "outputId": "b2c2fadb-b553-4c28-b4b9-5ccfc59e1795"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "\n",
    "    # Enable line-wrapping\n",
    "    # from IPython.display import HTML, display\n",
    "\n",
    "    # def set_css():\n",
    "    #     display(HTML('''\n",
    "    #         <style>\n",
    "    #         pre {\n",
    "    #             white-space: pre-wrap;\n",
    "    #             }\n",
    "    #         </style>\n",
    "    #     '''))\n",
    "    # get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # # Install python libraries in google drive once and then we can load it \n",
    "    # # from our area every time, we don't have do the reinstall every time\n",
    "    # LIBRARIES_PATH = '/content/drive/MyDrive/ML/lecr/code/libraries/'\n",
    "\n",
    "    # INSTALL_LIBRARIES = False # Set to True only once during first install\n",
    "    # if INSTALL_LIBRARIES:\n",
    "    #     %pip install --target=$LIBRARIES_PATH transformers\n",
    "    #     %pip install --target=$LIBRARIES_PATH datasets\n",
    "    #     %pip install --target=$LIBRARIES_PATH pynvml\n",
    "\n",
    "    # sys.path.append(LIBRARIES_PATH)\n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "WzPqPDg6MFLS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "WzPqPDg6MFLS",
    "outputId": "310431db-fb8d-4336-b181-adeafadfbfac"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Enable line-wrapping\n",
    "    from IPython.display import HTML, display\n",
    "\n",
    "    def set_css():\n",
    "        display(HTML('''\n",
    "            <style>\n",
    "            pre {\n",
    "                white-space: pre-wrap;\n",
    "                }\n",
    "            </style>\n",
    "        '''))\n",
    "    get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb01cd0b-b30b-41f8-ab3c-c963f1e6f482",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "collapsed": false,
    "id": "eb01cd0b-b30b-41f8-ab3c-c963f1e6f482",
    "outputId": "5b59ae55-fe46-482f-fdd2-31a67b462bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "#from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torchsummary\n",
    "from functools import partialmethod\n",
    "from itertools import product\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append('/content/drive/MyDrive/ML/lecr/code/')\n",
    "from l_utils import *\n",
    "from model_utils import *\n",
    "\n",
    "#from model_defs import Topic_NN, Content_NN\n",
    "\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "P0T5isInFStC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "P0T5isInFStC",
    "outputId": "965e6b9e-9df4-45bb-f27e-6be009d5d8e3"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    class CFG:\n",
    "        # MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        # MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        DIRNAME_PROCESSED_DATA = '/content/drive/MyDrive/ML/lecr/data/'\n",
    "        FILENAME_DATA_CONTENT_F = DIRNAME_PROCESSED_DATA + \"content.f\"\n",
    "        FILENAME_DATA_TOPICS_F = DIRNAME_PROCESSED_DATA + \"topics.f\"\n",
    "        FILENAME_DATA_CORRELATIONS_CSV = DIRNAME_PROCESSED_DATA + \"correlations.csv\"\n",
    "        NUM_FOLDS = 5\n",
    "        SAVE_MODEL_DIR = \"/content/drive/MyDrive/ML/lecr/models/\"\n",
    "else:\n",
    "    from cfg import *\n",
    "CFG.MODEL = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "topics_tokenized_pq_file = CFG.DIRNAME_PROCESSED_DATA + \"topics_tokenized.pq\"\n",
    "contents_tokenized_pq_file = CFG.DIRNAME_PROCESSED_DATA + \"contents_tokenized.pq\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54Hk7HUL6VuP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "collapsed": false,
    "id": "54Hk7HUL6VuP",
    "outputId": "dab73b66-7195-4133-97de-79163674207a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 29 17:54:06 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   34C    P8    18W / 275W |   4287MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1173      G   /usr/lib/xorg/Xorg                 12MiB |\r\n",
      "|    0   N/A  N/A      1218      G   /usr/bin/sddm-greeter              14MiB |\r\n",
      "|    0   N/A  N/A    700802      C   ...-code/le-venv/bin/python3     4255MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f465bcbb-a100-484a-aa43-10160343f582",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "f465bcbb-a100-484a-aa43-10160343f582",
    "outputId": "9d0b6f33-1239-45be-fa66-1ca64a00efe6"
   },
   "outputs": [],
   "source": [
    "# Split data into folds\n",
    "\n",
    "# Feature engineering ... use same as the one used for bce-lightning\n",
    "\n",
    "# Training for a Top-k metric ... see if this performs better than directly using minilm embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c18badcd-b5d0-4c9c-bca3-04d16435ab88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "c18badcd-b5d0-4c9c-bca3-04d16435ab88",
    "outputId": "81eb5d79-417c-4ddc-a8e3-c82f8932cddd"
   },
   "outputs": [],
   "source": [
    "def progress_bar_control(show_progress_bars = True):\n",
    "    tqdm.__init__ = partialmethod(tqdm.__init__, disable=(not show_progress_bars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0de9240c-c7b5-4e65-8f42-1af4226552ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "0de9240c-c7b5-4e65-8f42-1af4226552ac",
    "outputId": "4e9ab1e1-dc03-4e44-aef9-7a6244ebbb0c"
   },
   "outputs": [],
   "source": [
    "contents_df = pd.read_feather(CFG.FILENAME_DATA_CONTENT_F)\n",
    "topics_df   = pd.read_feather(CFG.FILENAME_DATA_TOPICS_F)\n",
    "correlations_df =  pd.read_csv(CFG.FILENAME_DATA_CORRELATIONS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a1ec376-3ef7-4cdf-a84b-c1dbf894e847",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "collapsed": false,
    "id": "8a1ec376-3ef7-4cdf-a84b-c1dbf894e847",
    "outputId": "7fbc0eee-0ab3-4afc-c59e-5a3170e1b93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154047 entries, 0 to 154046\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   id                154047 non-null  object\n",
      " 1   title             154038 non-null  object\n",
      " 2   description       89456 non-null   object\n",
      " 3   kind              154047 non-null  object\n",
      " 4   text              74035 non-null   object\n",
      " 5   language          154047 non-null  object\n",
      " 6   copyright_holder  71821 non-null   object\n",
      " 7   license           74035 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 9.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76972 entries, 0 to 76971\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           76972 non-null  object\n",
      " 1   title        76970 non-null  object\n",
      " 2   description  34953 non-null  object\n",
      " 3   channel      76972 non-null  object\n",
      " 4   category     76972 non-null  object\n",
      " 5   level        76972 non-null  int64 \n",
      " 6   language     76972 non-null  object\n",
      " 7   parent       76801 non-null  object\n",
      " 8   has_content  76972 non-null  bool  \n",
      "dtypes: bool(1), int64(1), object(7)\n",
      "memory usage: 4.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61517 entries, 0 to 61516\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   topic_id     61517 non-null  object\n",
      " 1   content_ids  61517 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 961.3+ KB\n"
     ]
    }
   ],
   "source": [
    "contents_df.info()\n",
    "topics_df.info()\n",
    "correlations_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00d65818-f54e-44b6-9517-5ffc45149575",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "00d65818-f54e-44b6-9517-5ffc45149575",
    "outputId": "a47d04a1-ef75-409a-9941-b7effe0791e8"
   },
   "outputs": [],
   "source": [
    "topics_tokenized_df   = pd.read_parquet(topics_tokenized_pq_file)\n",
    "contents_tokenized_df = pd.read_parquet(contents_tokenized_pq_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb247507-6691-49a4-a156-39e36d9ae91d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "collapsed": false,
    "id": "eb247507-6691-49a4-a156-39e36d9ae91d",
    "outputId": "1432ac64-05c7-468a-aa17-cd5326056e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154047 entries, 0 to 154046\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   id                  154047 non-null  object\n",
      " 1   c_0_input_ids       154047 non-null  object\n",
      " 2   c_1_attention_mask  154047 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61517 entries, 0 to 61516\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  61517 non-null  object\n",
      " 1   t_0_input_ids       61517 non-null  object\n",
      " 2   t_1_attention_mask  61517 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "contents_tokenized_df.info()\n",
    "topics_tokenized_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6274ee15-516d-4afc-9e18-616f93876a44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "collapsed": false,
    "id": "6274ee15-516d-4afc-9e18-616f93876a44",
    "outputId": "6c4c5ee3-60a4-48ac-9717-ac720c737813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177290\n",
      "102629\n",
      "2023-03-29 17:54:09.149249 : Splitting non-source into 4 folds\n",
      "fold\n",
      "0.0    25667\n",
      "1.0    25677\n",
      "2.0    25663\n",
      "3.0    25622\n",
      "dtype: int64\n",
      "fold\n",
      "0.0    6070\n",
      "1.0    4971\n",
      "2.0    8493\n",
      "3.0    5469\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train_df_orig, val_df_orig = create_train_val_split_pos_corr(topics_df, contents_df, correlations_df, CFG.NUM_FOLDS, min_train_perc = 80.0, use_topic_trees = True, random_seed = 3) # pick a random seed that gives a good split\n",
    "train_df_orig, val_df_orig = create_train_val_split_pos_corr_from_kaggle(topics_df, correlations_df, num_splits = 4, val_fold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58c5cb7a-00fd-4db7-9855-2a58a889cadf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "58c5cb7a-00fd-4db7-9855-2a58a889cadf",
    "outputId": "37f9eb38-35f1-49f1-eab0-4c8499241342"
   },
   "outputs": [],
   "source": [
    "# ***** FOR TESTING ONLY !!!!!!!! ******\n",
    "if TESTING_SUBSET:\n",
    "    train_df = train_df_orig[:1000]\n",
    "    val_df = val_df_orig[:1000]\n",
    "else:\n",
    "    train_df = train_df_orig\n",
    "    val_df = val_df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3c4ee3f-3136-490b-b0bd-bfe974fe3647",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "collapsed": false,
    "id": "f3c4ee3f-3136-490b-b0bd-bfe974fe3647",
    "outputId": "1eec41f2-bf5f-4add-b12a-51a4f7be31b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   t_id    1000 non-null   object\n",
      " 1   c_id    1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   t_id    1000 non-null   object\n",
      " 1   c_id    1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26771d02-f345-4a94-9508-1973435fd85e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "collapsed": false,
    "id": "26771d02-f345-4a94-9508-1973435fd85e",
    "outputId": "66178cc5-4f2a-4f02-f89b-0781d7e6d60a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [t_id, c_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df.loc[random.sample(range(0,len(train_df)),10)]\n",
    "train_df.loc[train_df['t_id']=='t_f2872cd7562c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ff6f76a-fe1b-4fcb-816b-cf857c7942e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "4ff6f76a-fe1b-4fcb-816b-cf857c7942e5",
    "outputId": "69d5c82a-dd8d-4efa-a2be-bd9173240bac"
   },
   "outputs": [],
   "source": [
    "PARTITION_BATCH_SIZE = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cbc1c7a-4059-4559-a836-a8f1fb22cbd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "collapsed": false,
    "id": "4cbc1c7a-4059-4559-a836-a8f1fb22cbd2",
    "outputId": "101dc8e0-7fdc-4a6a-c1a5-c46d01aa1f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219 entries, 0 to 218\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   t_id    219 non-null    object\n",
      " 1   label   219 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   c_id    1000 non-null   object\n",
      " 1   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "2023-03-29 17:54:09.352856 : Merging in topic and content data into the folds data ... \n"
     ]
    }
   ],
   "source": [
    "# train_tokenized_df = get_partitioned_data_for_pos_corr(train_df, topics_tokenized_df, contents_tokenized_df, batch_size = PARTITION_BATCH_SIZE, do_partitioning = False, sort = False)\n",
    "train_tokenized_df, output_classes = get_labeled_data_for_pos_corr(train_df.copy(), topics_tokenized_df, contents_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15fa9882-4603-4860-a850-be25242d330e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "collapsed": false,
    "id": "15fa9882-4603-4860-a850-be25242d330e",
    "outputId": "bf87d397-222b-4cf2-c15a-d64aa0f9af11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1219 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   label           1219 non-null   int64 \n",
      " 1   input_ids       1219 non-null   object\n",
      " 2   attention_mask  1219 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 38.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "zkMN4UzVPN3z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "zkMN4UzVPN3z",
    "outputId": "ec93e4b1-aec7-429d-d301-d9aa83256889"
   },
   "outputs": [],
   "source": [
    "val_topics_tokenized_df, val_contents_tokenized_df, val_corr_df = get_pos_corr_subsets_for_binary_fold( val_df, contents_tokenized_df, topics_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3dbcf76e-d5b9-480d-bfb4-6d838d3610ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "collapsed": false,
    "id": "3dbcf76e-d5b9-480d-bfb4-6d838d3610ab",
    "outputId": "73697ebe-1e8e-4cdd-d8e1-e48afbba347c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239 entries, 0 to 238\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   index               239 non-null    int64 \n",
      " 1   id                  239 non-null    object\n",
      " 2   t_0_input_ids       239 non-null    object\n",
      " 3   t_1_attention_mask  239 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 7.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 984 entries, 0 to 983\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   index               984 non-null    int64 \n",
      " 1   id                  984 non-null    object\n",
      " 2   c_0_input_ids       984 non-null    object\n",
      " 3   c_1_attention_mask  984 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 30.9+ KB\n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 239 entries, 0 to 238\n",
      "Series name: c_idx\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "239 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.7+ KB\n"
     ]
    }
   ],
   "source": [
    "val_topics_tokenized_df.info()\n",
    "val_contents_tokenized_df.info()\n",
    "val_corr_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df4bea1f-564e-4079-b646-f2e68a70219e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "df4bea1f-564e-4079-b646-f2e68a70219e",
    "outputId": "5a88c699-989e-4b6e-8dc9-4819ef3b7a3b"
   },
   "outputs": [],
   "source": [
    "class TransModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper. Gets tokenized features and returns the output logits for whether the token and content are matching.\n",
    "    \"\"\"\n",
    "    def __init__(self, trans_model_name):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(trans_model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(token_embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "        Average the output embeddings using the attention mask \n",
    "        to ignore certain tokens.\n",
    "        \"\"\"\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "        )\n",
    "\n",
    "    def get_transformer_output(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids = input_ids, attention_mask=attention_mask)\n",
    "        pooled_embeddings=self.mean_pooling(outputs[0], attention_mask)\n",
    "        return pooled_embeddings\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Grab embeddings from transformer\n",
    "        embs = self.get_transformer_output(\n",
    "            features[:,0,:],\n",
    "            features[:,1,:]\n",
    "        )\n",
    "\n",
    "        return embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2832046-c9b2-4e07-8abb-e942f4ec4f3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "e2832046-c9b2-4e07-8abb-e942f4ec4f3a",
    "outputId": "ce8addae-9828-408b-9174-be030830480f"
   },
   "outputs": [],
   "source": [
    "class CustomDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_tokenized_df):\n",
    "        feature_cols = [col for col in train_tokenized_df.columns if 'label' not in col]\n",
    "        self.features = torch.tensor(train_tokenized_df[feature_cols].values.tolist())\n",
    "        self.label   = torch.tensor(train_tokenized_df[['label']].values.tolist())\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.features.size(dim=0)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        features = self.features[item]\n",
    "        label    = self.label[item]\n",
    "\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f21d2d8-257d-4707-a36c-d50959fc31fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "collapsed": false,
    "id": "0f21d2d8-257d-4707-a36c-d50959fc31fa",
    "outputId": "656631e9-61a9-46e0-bb47-44dca3a6d525"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_ds = CustomDS(train_tokenized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "024caad2-93b8-4f92-b89f-e200a5d1c009",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": false,
    "id": "024caad2-93b8-4f92-b89f-e200a5d1c009",
    "outputId": "f934fe40-2c46-402e-8306-a59678098dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1219, 2, 64])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89042ca8-d9ab-4279-9244-ea01f0b50f70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "collapsed": false,
    "id": "89042ca8-d9ab-4279-9244-ea01f0b50f70",
    "outputId": "efad502d-f176-49dc-fb56-a38e0851f040"
   },
   "outputs": [],
   "source": [
    "t_cols = [col for col in val_topics_tokenized_df.columns if col.startswith('t_')]\n",
    "c_cols = [col for col in val_contents_tokenized_df.columns if col.startswith('c_')]\n",
    "val_data = (torch.tensor(val_topics_tokenized_df[t_cols].values.tolist()),\n",
    "            torch.tensor(val_contents_tokenized_df[c_cols].values.tolist())\n",
    "            ,val_corr_df\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "436d52f2-8ad3-419b-bb96-c6acda12755b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "collapsed": false,
    "id": "436d52f2-8ad3-419b-bb96-c6acda12755b",
    "outputId": "a88b8b53-4b21-4e88-ba16-2b50521d15cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0\n",
      "process     700802 uses     4255.000 MB GPU memory\n"
     ]
    }
   ],
   "source": [
    "try: trans_model\n",
    "except NameError: trans_model = None\n",
    "if trans_model is not None:\n",
    "  del trans_model\n",
    "report_gpu()\n",
    "if TESTING_SUBSET:\n",
    "    # 20 works good for training for TESTING_SUBSET\n",
    "    BATCH_SIZE = 20\n",
    "else:\n",
    "    # BATCH_SIZE = 1100 # Works during first round of training, but if we do a reload of saved model, this number causes out of memory at 2nd epoch\n",
    "    BATCH_SIZE = 1050 # \n",
    "    # 1875 works for arccos, 1950 works but runs out of memory after few epochs\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09b2154a-d3b0-4dc7-92ab-f8d16c8cf88e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": false,
    "id": "09b2154a-d3b0-4dc7-92ab-f8d16c8cf88e",
    "outputId": "64cd13b0-d5f9-4eb7-ecc8-07b4ad9a6d6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mahesh/.cache/torch/hub/adeelh_pytorch-multi-class-focal-loss_master\n"
     ]
    }
   ],
   "source": [
    "# Add option to load pre-existing model\n",
    "LOAD_CHECKPOINT_MODEL = False\n",
    "SAVE_BEST_MODEL = True\n",
    "DRY_RUN = False\n",
    "TRAIN_PATIENCE = 5\n",
    "k_vals = [5,10, 50]\n",
    "METRIC = 'recall'\n",
    "METRIC_KEY = 10\n",
    "MARGIN = 0.4\n",
    "GAMMA = 2.0\n",
    "MODEL_FILENAME_PREFIX = f'one_trans_distiluse_ArcFace_{MARGIN}_focal_loss_{GAMMA}_fp16_ml64_train_shuffled_parent_title_new_sep_unique_id_batch_size_{BATCH_SIZE}'\n",
    "SAVE_MODEL_DIR = CFG.SAVE_MODEL_DIR\n",
    "# TRANS_OUTPUT_EMBS = 768\n",
    "trans_model = TransModel( trans_model_name = CFG.MODEL)\n",
    "focal_loss = torch.hub.load(\n",
    "\t'adeelh/pytorch-multi-class-focal-loss',\n",
    "\tmodel='focal_loss',\n",
    "\t# alpha=[.75, .25],\n",
    "\tgamma=GAMMA,\n",
    "\treduction='mean',\n",
    "\tdevice=device,\n",
    "\tdtype=torch.float32,\n",
    "\tforce_reload=False,\n",
    "    trust_repo=True\n",
    ")\n",
    "TRANS_OUTPUT_EMBS = trans_model.transformer.pooler.dense.out_features\n",
    "criterion1 = ArcosLossWithWeights(TRANS_OUTPUT_EMBS, output_classes, loss_function = focal_loss, margin=MARGIN)\n",
    "optimizer = torch.optim.Adam((list(trans_model.parameters())) + (list(criterion1.parameters())),\n",
    "                              lr=2e-4,\n",
    "                             #weight_decay = 1e-4 # L2 regularization, 1e-4 is a good value\n",
    "                             )\n",
    "# criterion0 = MultipleNegativesRankingLoss()\n",
    "model_dict = {'trans_model': trans_model, 'optimizer': optimizer, 'criterion':criterion1}\n",
    "if LOAD_CHECKPOINT_MODEL:\n",
    "    # MODEL_FILENAME = SAVE_MODEL_DIR + 'one_trans_MiniLM_train_shuffled_parent_title_epoch_5_recall_50_0.904.pth'\n",
    "    # t_model, _, _ = load_trans_model(t_model, criterion = None, optimizer = None, filename = MODEL_FILENAME)\n",
    "    # train_scores = []\n",
    "    # val_scores = []\n",
    "    # epoch_count = 0\n",
    "    # MODEL_FILENAME = None\n",
    "\n",
    "    MODEL_FILENAME = SAVE_MODEL_DIR + 'one_trans_distiluse_MNRL_pretrained_ArcFace_0.4_focal_loss_2.0_fp16_ml64_train_shuffled_parent_title_new_sep_unique_id_batch_size_1050_epoch_35_recall_10_0.778.pth'\n",
    "    load_model( model_dict, filename = MODEL_FILENAME)\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    # criterion1.margin = 0.6\n",
    "\n",
    "\n",
    "    # TRAIN_DATA_FILENAME = SAVE_MODEL_DIR + f'{MODEL_FILENAME_PREFIX}.train.txt'\n",
    "    # TRAIN_DATA_FILENAME = SAVE_MODEL_DIR + 'one_trans_mpnet_MNRL_pretrained_ArcFace_0.4_focal_loss_2.0_fp16_ml64_train_shuffled_parent_title_new_sep_batch_size_1100.train.txt'\n",
    "    # train_data_dict = load_dict(TRAIN_DATA_FILENAME)\n",
    "    # train_scores, val_scores = train_data_dict['train_scores'], train_data_dict['val_scores']\n",
    "    # epoch_count = len(val_scores)\n",
    "\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    epoch_count = 0\n",
    "    MODEL_FILENAME = None\n",
    "    \n",
    "else:\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    epoch_count = 0\n",
    "    MODEL_FILENAME = None\n",
    "\n",
    "trans_model = trans_model.to(device)\n",
    "criterion1 = criterion1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a443487-0abf-46f5-9544-828b0795fcf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "id": "8a443487-0abf-46f5-9544-828b0795fcf5",
    "outputId": "005221f3-0f40-4400-c46d-af8f75df9683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-29 18:01:26.651772 : Starting Training\n",
      "2023-03-29 18:01:26.652690 : ***** Epoch 0 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0% 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0% 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autocast(device_type=<span style=\"color: #808000; text-decoration-color: #808000\">'cuda'</span>, dtype=torch.float16):                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 │   │   │   </span>t_emb = trans_model(features)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss1 = criterion1(t_emb, label)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 │   │   # loss.backward()</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 │   │   # optimizer.step()</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/le-venv/lib/python3.10/si</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">te-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">172</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, embeddings, labels, debug = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>):                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   │   # Step 1: Get cos-similarity </span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   │   </span>cos_sim_scores = (                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>172 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>cos_sim(embeddings, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.W)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   # this prevents nan when a value slightly crosses 1.0 due to numerical error</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">57</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cos_sim</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 54 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 55 │   </span>a_norm = torch.nn.functional.normalize(a, p=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 56 │   </span>b_norm = torch.nn.functional.normalize(b, p=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 57 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.mm(a_norm, b_norm.transpose(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>))                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 58 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 59 # Basically the same as this: https://github.com/UKPLab/sentence-transformers/blob/maste</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 60 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">MultipleNegativesRankingLoss</span>(torch.nn.Module):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>mat1 and mat2 shapes cannot be multiplied <span style=\"font-weight: bold\">(</span>2<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x768</span> and 384x219<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m torch.autocast(device_type=\u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m, dtype=torch.float16):                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0mt_emb = trans_model(features)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m27 \u001b[2m│   │   │   \u001b[0mloss1 = criterion1(t_emb, label)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# loss.backward()\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# optimizer.step()\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/le-venv/lib/python3.10/si\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mte-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/\u001b[0m\u001b[1;33mmodel_utils.py\u001b[0m:\u001b[94m172\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, embeddings, labels, debug = \u001b[94mFalse\u001b[0m):                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Step 1: Get cos-similarity \u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   \u001b[0mcos_sim_scores = (                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m172 \u001b[2m│   │   │   \u001b[0mcos_sim(embeddings, \u001b[96mself\u001b[0m.W)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this prevents nan when a value slightly crosses 1.0 due to numerical error\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/\u001b[0m\u001b[1;33mmodel_utils.py\u001b[0m:\u001b[94m57\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mcos_sim\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 54 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 55 \u001b[0m\u001b[2m│   \u001b[0ma_norm = torch.nn.functional.normalize(a, p=\u001b[94m2\u001b[0m, dim=\u001b[94m1\u001b[0m)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 56 \u001b[0m\u001b[2m│   \u001b[0mb_norm = torch.nn.functional.normalize(b, p=\u001b[94m2\u001b[0m, dim=\u001b[94m1\u001b[0m)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 57 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch.mm(a_norm, b_norm.transpose(\u001b[94m0\u001b[0m, \u001b[94m1\u001b[0m))                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 58 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 59 \u001b[0m\u001b[2m# Basically the same as this: https://github.com/UKPLab/sentence-transformers/blob/maste\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 60 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mMultipleNegativesRankingLoss\u001b[0m(torch.nn.Module):                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mmat1 and mat2 shapes cannot be multiplied \u001b[1m(\u001b[0m2\u001b[1;36m0x768\u001b[0m and 384x219\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x768 and 384x219)",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "\n",
    "DISABLE_TQDM = False\n",
    "if (DISABLE_TQDM):\n",
    "    tqdm.__init__ = partialmethod(tqdm.__init__, disable=DISABLE_TQDM)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "pt('Starting Training')\n",
    "trans_model.train()\n",
    "# c_model.train()\n",
    "#for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # We set the below in case it is frozen while doing eval\n",
    "    pt(f\"***** Epoch {epoch_count} *****\")\n",
    "    train_loss = 0\n",
    "    #train_acc = 0\n",
    "    #train_f2_score = 0\n",
    "    for (features, label) in tqdm(train_loader, desc=\"Training\"):\n",
    "        features = features.to(device)\n",
    "        label    = label.to(device).squeeze(1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            t_emb = trans_model(features)\n",
    "            loss1 = criterion1(t_emb, label)\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        scaler.scale(loss1).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "       \n",
    "        train_loss += loss1.item()\n",
    "    train_loss = round(train_loss/len(train_loader), 5)\n",
    "    pt(f'Train : | Loss: {train_loss} ')\n",
    "    train_scores.append({'loss':train_loss})\n",
    "    recalls  = compute_recall_at_k_for_two_trans(trans_model, trans_model, device, val_data[0], val_data[1], val_data[2], k_vals = k_vals, batch_size = BATCH_SIZE)\n",
    "    recall_str = '\\nValidation:  ' + ''.join([f'| Recall@{k}: {recalls[k]:.3f}' for k in k_vals])\n",
    "    pt(recall_str)\n",
    "    val_scores.append({'recall':recalls})\n",
    "    if SAVE_BEST_MODEL:\n",
    "        MODEL_FILENAME = save_best_model( \n",
    "                                        model_dict = model_dict,\n",
    "                                        old_model_filename = MODEL_FILENAME,\n",
    "                                        model_filename_prefix = MODEL_FILENAME_PREFIX,\n",
    "                                        model_path = SAVE_MODEL_DIR,\n",
    "                                        save_model_fn = save_model,\n",
    "                                        train_scores = train_scores,\n",
    "                                        val_scores = val_scores, \n",
    "                                        metric = METRIC, metric_key = METRIC_KEY,\n",
    "                                        dry_run = DRY_RUN\n",
    "                                        )\n",
    "    save_model(model_dict, SAVE_MODEL_DIR + f\"{MODEL_FILENAME_PREFIX}.last_model.pth\")\n",
    "    epoch_count += 1\n",
    "    save_dict({'train_scores':train_scores, 'val_scores':val_scores}, SAVE_MODEL_DIR + f'{MODEL_FILENAME_PREFIX}.train.txt')\n",
    "    if (TRAIN_PATIENCE >=0  and (not check_improvement(val_scores, metric = METRIC, metric_key = METRIC_KEY, patience = TRAIN_PATIENCE))):\n",
    "        pt(f'Exiting training since no improvement in the last {TRAIN_PATIENCE} epochs')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "-9y5cb8CWuZO",
   "metadata": {
    "collapsed": false,
    "id": "-9y5cb8CWuZO"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "argv": [
    "/home/mahesh/Desktop/ML/kaggle/LearningEquality/learning-equality-code/le-venv/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "le-venv",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "le-venv"
  },
  "language_info": {
   "name": "python"
  },
  "name": "One_trans_arccos_loss_no_lightning_train_shuffled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
